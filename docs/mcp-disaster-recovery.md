# ðŸ›¡ï¸ MCP Disaster Recovery & Business Continuity

## 1. RTO/RPO Definitions

```yaml
service_tiers:
  critical:
    rto: 15min  # Recovery Time Objective
    rpo: 5min   # Recovery Point Objective
    services: [auth, payments, core-api]
  
  standard:
    rto: 1hour
    rpo: 30min
    services: [analytics, reporting]
  
  low:
    rto: 4hours
    rpo: 4hours
    services: [batch-jobs, maintenance]
```

## 2. Backup Strategy

### PostgreSQL Backup

```yaml
# deploy/backup/postgres-backup.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:16
            command:
            - /bin/bash
            - -c
            - |
              DATE=$(date +%Y%m%d_%H%M%S)
              BACKUP_FILE="backup_${DATE}.sql.gz"
              
              # Backup with WAL
              pg_basebackup -h $POSTGRES_HOST -U $POSTGRES_USER \
                -D /backup/base_${DATE} -Ft -z -P -Xs
              
              # Point-in-time recovery setup
              pg_dump -h $POSTGRES_HOST -U $POSTGRES_USER \
                --no-owner --clean --if-exists \
                $POSTGRES_DB | gzip > /backup/${BACKUP_FILE}
              
              # Upload to S3 with versioning
              aws s3 cp /backup/${BACKUP_FILE} \
                s3://${BACKUP_BUCKET}/postgres/${BACKUP_FILE} \
                --storage-class GLACIER_IR
              
              # Validate backup
              gunzip -c /backup/${BACKUP_FILE} | head -100 > /dev/null
              if [ $? -eq 0 ]; then
                echo "Backup validated successfully"
              else
                exit 1
              fi
              
              # Cleanup old local backups (keep 7 days)
              find /backup -name "*.sql.gz" -mtime +7 -delete
```

### NATS JetStream Backup

```go
// internal/backup/nats_backup.go
package backup

import (
    "context"
    "fmt"
    "time"
    "github.com/nats-io/nats.go"
)

type JetStreamBackup struct {
    js nats.JetStreamContext
    s3 S3Client
}

func (b *JetStreamBackup) BackupStream(ctx context.Context, streamName string) error {
    // Get stream info
    info, err := b.js.StreamInfo(streamName)
    if err != nil {
        return fmt.Errorf("get stream info: %w", err)
    }
    
    // Create snapshot
    snapshot := &StreamSnapshot{
        Name:      streamName,
        Timestamp: time.Now(),
        Config:    info.Config,
        State:     info.State,
    }
    
    // Backup messages
    consumer, err := b.js.CreateConsumer(streamName, &nats.ConsumerConfig{
        Durable:       "backup-consumer",
        DeliverPolicy: nats.DeliverAllPolicy,
        AckPolicy:     nats.AckNonePolicy,
    })
    if err != nil {
        return fmt.Errorf("create consumer: %w", err)
    }
    
    messages := make([]StoredMessage, 0)
    sub, err := consumer.Subscribe(func(msg *nats.Msg) {
        messages = append(messages, StoredMessage{
            Subject:  msg.Subject,
            Data:     msg.Data,
            Headers:  msg.Header,
            Sequence: msg.Sequence,
        })
    })
    if err != nil {
        return fmt.Errorf("subscribe: %w", err)
    }
    defer sub.Unsubscribe()
    
    // Wait for all messages
    time.Sleep(5 * time.Second)
    snapshot.Messages = messages
    
    // Upload to S3
    backupKey := fmt.Sprintf("nats/%s/backup_%d.json", 
        streamName, time.Now().Unix())
    
    return b.s3.Upload(ctx, backupKey, snapshot)
}

func (b *JetStreamBackup) RestoreStream(ctx context.Context, backupKey string) error {
    // Download from S3
    var snapshot StreamSnapshot
    if err := b.s3.Download(ctx, backupKey, &snapshot); err != nil {
        return fmt.Errorf("download backup: %w", err)
    }
    
    // Recreate stream
    _, err := b.js.AddStream(&snapshot.Config)
    if err != nil && err != nats.ErrStreamNameAlreadyInUse {
        return fmt.Errorf("add stream: %w", err)
    }
    
    // Restore messages
    for _, msg := range snapshot.Messages {
        _, err := b.js.PublishMsg(&nats.Msg{
            Subject: msg.Subject,
            Data:    msg.Data,
            Header:  msg.Headers,
        })
        if err != nil {
            return fmt.Errorf("publish message: %w", err)
        }
    }
    
    return nil
}
```

## 3. Multi-Region Failover

```yaml
# deploy/dr/multi-region.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-config
data:
  regions: |
    primary:
      name: us-east-1
      endpoints:
        postgres: postgres-primary.us-east-1.rds.amazonaws.com
        nats: nats-cluster-us-east-1.example.com
        redis: redis-primary.us-east-1.cache.amazonaws.com
    
    secondary:
      name: eu-west-1
      endpoints:
        postgres: postgres-replica.eu-west-1.rds.amazonaws.com
        nats: nats-cluster-eu-west-1.example.com
        redis: redis-replica.eu-west-1.cache.amazonaws.com
    
    dr:
      name: us-west-2
      endpoints:
        postgres: postgres-dr.us-west-2.rds.amazonaws.com
        nats: nats-cluster-us-west-2.example.com
        redis: redis-dr.us-west-2.cache.amazonaws.com
```

### Failover Controller

```go
// internal/dr/failover.go
package dr

import (
    "context"
    "time"
)

type FailoverController struct {
    regions      []Region
    healthChecks map[string]HealthChecker
    current      *Region
    metrics      *FailoverMetrics
}

func (fc *FailoverController) MonitorAndFailover(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    consecutiveFailures := 0
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            if fc.isHealthy(fc.current) {
                consecutiveFailures = 0
                fc.metrics.RecordHealthCheck(fc.current.Name, true)
                continue
            }
            
            fc.metrics.RecordHealthCheck(fc.current.Name, false)
            consecutiveFailures++
            
            if consecutiveFailures >= 3 {
                if err := fc.initiateFailover(ctx); err != nil {
                    fc.metrics.RecordFailoverError(err)
                } else {
                    fc.metrics.RecordFailoverSuccess()
                }
                consecutiveFailures = 0
            }
        }
    }
}

func (fc *FailoverController) initiateFailover(ctx context.Context) error {
    startTime := time.Now()
    
    // 1. Find healthy region
    var targetRegion *Region
    for _, region := range fc.regions {
        if region.Name != fc.current.Name && fc.isHealthy(&region) {
            targetRegion = &region
            break
        }
    }
    
    if targetRegion == nil {
        return ErrNoHealthyRegion
    }
    
    // 2. Pre-failover checks
    if err := fc.preFailoverChecks(targetRegion); err != nil {
        return fmt.Errorf("pre-failover checks failed: %w", err)
    }
    
    // 3. Update DNS
    if err := fc.updateDNS(targetRegion); err != nil {
        return fmt.Errorf("DNS update failed: %w", err)
    }
    
    // 4. Drain connections
    if err := fc.drainConnections(fc.current, 30*time.Second); err != nil {
        log.Printf("Warning: drain failed: %v", err)
    }
    
    // 5. Switch traffic
    oldRegion := fc.current.Name
    fc.current = targetRegion
    
    // 6. Verify
    if !fc.verifyFailover(targetRegion) {
        return ErrFailoverVerificationFailed
    }
    
    // 7. Record metrics
    duration := time.Since(startTime)
    fc.metrics.RecordFailoverDuration(duration)
    
    // 8. Alert
    fc.sendAlert(FailoverCompleted{
        From:      oldRegion,
        To:        targetRegion.Name,
        Timestamp: time.Now(),
        Duration:  duration,
        Reason:    "Health check failures",
    })
    
    return nil
}

func (fc *FailoverController) preFailoverChecks(target *Region) error {
    checks := []struct {
        name string
        fn   func() error
    }{
        {"database_connectivity", func() error {
            return fc.checkDatabaseConnection(target.Endpoints.Postgres)
        }},
        {"replication_lag", func() error {
            lag, err := fc.getReplicationLag(target.Endpoints.Postgres)
            if err != nil {
                return err
            }
            if lag > 30*time.Second {
                return fmt.Errorf("replication lag too high: %v", lag)
            }
            return nil
        }},
        {"nats_cluster_health", func() error {
            return fc.checkNATSCluster(target.Endpoints.NATS)
        }},
        {"redis_availability", func() error {
            return fc.checkRedis(target.Endpoints.Redis)
        }},
    }
    
    for _, check := range checks {
        if err := check.fn(); err != nil {
            return fmt.Errorf("%s: %w", check.name, err)
        }
    }
    
    return nil
}
```

## 4. Data Replication Strategy

```yaml
# PostgreSQL Streaming Replication
replication:
  mode: streaming
  max_wal_senders: 10
  wal_level: replica
  hot_standby: true
  archive_mode: on
  archive_command: 'aws s3 cp %p s3://wal-archive/%f'
  
  replicas:
    - name: replica-1
      region: us-east-1b
      lag_threshold: 5s
      priority: 100
    - name: replica-2
      region: eu-west-1a
      lag_threshold: 10s
      priority: 90
    - name: dr-replica
      region: us-west-2a
      lag_threshold: 30s
      priority: 50
```

### Replication Monitor

```go
// internal/dr/replication_monitor.go
package dr

type ReplicationMonitor struct {
    db      *sql.DB
    metrics *ReplicationMetrics
}

func (rm *ReplicationMonitor) CheckReplicationStatus(ctx context.Context) (*ReplicationStatus, error) {
    var status ReplicationStatus
    
    // Check replication lag
    query := `
        SELECT 
            client_addr,
            state,
            sent_lsn,
            write_lsn,
            flush_lsn,
            replay_lsn,
            EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag_seconds
        FROM pg_stat_replication
    `
    
    rows, err := rm.db.QueryContext(ctx, query)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    for rows.Next() {
        var replica ReplicaStatus
        err := rows.Scan(
            &replica.Address,
            &replica.State,
            &replica.SentLSN,
            &replica.WriteLSN,
            &replica.FlushLSN,
            &replica.ReplayLSN,
            &replica.LagSeconds,
        )
        if err != nil {
            continue
        }
        
        replica.LagBytes = rm.calculateLagBytes(replica.SentLSN, replica.ReplayLSN)
        status.Replicas = append(status.Replicas, replica)
        
        // Record metrics
        rm.metrics.RecordReplicationLag(replica.Address, replica.LagSeconds)
    }
    
    return &status, nil
}
```

## 5. Chaos Testing for DR

```go
// test/chaos/dr_test.go
package chaos

import (
    "testing"
    "time"
    "github.com/stretchr/testify/assert"
)

func TestDisasterRecovery(t *testing.T) {
    // Test primary region failure
    t.Run("PrimaryRegionFailure", func(t *testing.T) {
        // Setup multi-region environment
        env := setupMultiRegionEnv(t)
        defer env.Cleanup()
        
        // Start continuous workload
        workload := startContinuousWorkload(env.Primary)
        defer workload.Stop()
        
        // Record initial state
        initialTransactions := workload.GetCompletedTransactions()
        
        // Kill primary region
        env.Primary.Kill()
        killTime := time.Now()
        
        // Verify automatic failover
        assert.Eventually(t, func() bool {
            return env.Secondary.IsActive()
        }, 2*time.Minute, 10*time.Second, "Failover should complete within 2 minutes")
        
        // Calculate recovery time
        recoveryTime := time.Since(killTime)
        
        // Verify RTO compliance
        assert.LessOrEqual(t, recoveryTime, 15*time.Minute, 
            "Recovery time should be within RTO of 15 minutes")
        
        // Verify RPO compliance (data loss)
        finalTransactions := workload.GetCompletedTransactions()
        lostTransactions := workload.GetLostTransactions(initialTransactions, finalTransactions)
        
        // Assuming 100 tx/sec, 5 min RPO = max 30000 lost transactions
        assert.LessOrEqual(t, len(lostTransactions), 30000, 
            "Data loss should be within RPO of 5 minutes")
        
        // Verify data consistency
        assert.True(t, env.Secondary.VerifyDataIntegrity(), 
            "Secondary should have consistent data")
    })
    
    // Test split-brain scenario
    t.Run("SplitBrainPrevention", func(t *testing.T) {
        env := setupMultiRegionEnv(t)
        defer env.Cleanup()
        
        // Partition network between regions
        env.CreateNetworkPartition("primary", "secondary")
        
        // Both regions should not accept writes simultaneously
        primaryAccepts := env.Primary.AcceptsWrites()
        secondaryAccepts := env.Secondary.AcceptsWrites()
        
        assert.False(t, primaryAccepts && secondaryAccepts, 
            "Split-brain: both regions accepting writes")
    })
}
```

## 6. Runbooks - Automated Response

### Database Corruption Recovery

```bash
#!/bin/bash
# runbooks/db-corruption-recovery.sh

set -euo pipefail

echo "ðŸš¨ Database Corruption Recovery Started at $(date)"

# Configuration
SLACK_WEBHOOK="${SLACK_WEBHOOK_URL}"
PRIMARY_DB="${PRIMARY_DB_HOST}"
BACKUP_BUCKET="${BACKUP_S3_BUCKET}"

# Functions
notify() {
    curl -X POST $SLACK_WEBHOOK -d "{\"text\":\"$1\"}"
}

# 1. Detect and isolate corruption
echo "Step 1: Isolating corrupted instance..."
kubectl cordon postgres-primary
kubectl label pod postgres-primary status=corrupted --overwrite
notify "ðŸ“ DR: Primary database isolated due to corruption"

# 2. Promote best replica
echo "Step 2: Finding best replica to promote..."
BEST_REPLICA=$(psql -h monitoring-db -t -c "
    SELECT client_addr FROM pg_stat_replication 
    WHERE state = 'streaming' 
    ORDER BY replay_lsn DESC 
    LIMIT 1
")

echo "Promoting replica: $BEST_REPLICA"
kubectl exec postgres-replica-1 -- pg_ctl promote
notify "ðŸ”„ DR: Promoted $BEST_REPLICA to primary"

# 3. Update service endpoints
echo "Step 3: Updating service endpoints..."
kubectl patch service postgres -p \
  '{"spec":{"selector":{"role":"replica-1"}}}'

# 4. Find latest valid backup
echo "Step 4: Identifying recovery point..."
LATEST_BACKUP=$(aws s3api list-objects-v2 \
    --bucket $BACKUP_BUCKET \
    --prefix postgres/ \
    --query 'Contents[?contains(Key, `.sql.gz`)]|[?LastModified >= `'$(date -u -d '1 day ago' '+%Y-%m-%dT%H:%M:%S')'`].Key' \
    --output text | tail -1)

echo "Latest backup: $LATEST_BACKUP"

# 5. Restore corrupted instance for investigation
echo "Step 5: Restoring corrupted instance for forensics..."
aws s3 cp s3://$BACKUP_BUCKET/$LATEST_BACKUP /tmp/recovery.sql.gz
gunzip /tmp/recovery.sql.gz

# 6. Verify data integrity
echo "Step 6: Verifying data integrity..."
CHECKSUM_NEW=$(psql -h $BEST_REPLICA -t -c "SELECT md5(string_agg(md5(t::text), '')) FROM critical_table t")
echo "Data checksum: $CHECKSUM_NEW"

# 7. Update monitoring
echo "Step 7: Updating monitoring and alerts..."
curl -X POST http://prometheus-pushgateway:9091/metrics/job/dr_recovery \
    --data-binary @- <<EOF
dr_recovery_completed{type="corruption"} 1
dr_recovery_duration_seconds $(date +%s)
dr_data_loss_bytes 0
EOF

notify "âœ… DR: Database corruption recovery completed. RTO: $(date +%s)s"
echo "âœ… Recovery completed successfully at $(date)"
```

### Region Failover Runbook

```yaml
# runbooks/region-failover.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: region-failover-runbook
data:
  automated-procedure.sh: |
    #!/bin/bash
    
    # Automated Region Failover Procedure
    # Triggered by: 3 consecutive health check failures
    
    INCIDENT_ID=$(uuidgen)
    START_TIME=$(date +%s)
    
    # 1. DETECT - Already triggered by monitoring
    log "INCIDENT:$INCIDENT_ID" "Region failure detected"
    
    # 2. ASSESS
    REGIONS=(us-east-1 eu-west-1 us-west-2)
    HEALTHY_REGION=""
    
    for region in "${REGIONS[@]}"; do
        if check_region_health "$region"; then
            HEALTHY_REGION=$region
            break
        fi
    done
    
    if [ -z "$HEALTHY_REGION" ]; then
        alert_critical "No healthy regions available!"
        exit 1
    fi
    
    # 3. EXECUTE FAILOVER
    log "INCIDENT:$INCIDENT_ID" "Failing over to $HEALTHY_REGION"
    
    # Update Route53 weighted routing
    aws route53 change-resource-record-sets \
        --hosted-zone-id $ZONE_ID \
        --change-batch '{
            "Changes": [{
                "Action": "UPSERT",
                "ResourceRecordSet": {
                    "Name": "api.example.com",
                    "Type": "A",
                    "AliasTarget": {
                        "HostedZoneId": "'$HEALTHY_REGION_ZONE'",
                        "DNSName": "'$HEALTHY_REGION_LB'",
                        "EvaluateTargetHealth": true
                    },
                    "SetIdentifier": "Primary",
                    "Weight": 100
                }
            }]
        }'
    
    # 4. VERIFY
    sleep 30
    if ! verify_endpoint "https://api.example.com/health"; then
        alert_critical "Failover verification failed!"
        rollback_failover
        exit 1
    fi
    
    # 5. COMMUNICATE
    END_TIME=$(date +%s)
    RTO=$((END_TIME - START_TIME))
    
    notify_stakeholders "
    ðŸ”„ Region Failover Completed
    - Incident: $INCIDENT_ID
    - Failed Region: $FAILED_REGION
    - New Primary: $HEALTHY_REGION
    - RTO: ${RTO}s
    - Status: Operational
    "
    
    # 6. MONITOR
    start_enhanced_monitoring $HEALTHY_REGION
    
    log "INCIDENT:$INCIDENT_ID" "Failover completed in ${RTO}s"
```

## 7. Backup Validation

```go
// internal/dr/backup_validator.go
package dr

import (
    "context"
    "crypto/md5"
    "fmt"
    "time"
)

type BackupValidator struct {
    s3Client S3Client
    db       *sql.DB
    metrics  *BackupMetrics
}

func (bv *BackupValidator) ValidateBackups(ctx context.Context) (*ValidationReport, error) {
    report := &ValidationReport{
        Timestamp: time.Now(),
        Backups:   make([]BackupValidation, 0),
    }
    
    // List recent backups
    backups, err := bv.s3Client.ListBackups(ctx, time.Now().Add(-24*time.Hour))
    if err != nil {
        return nil, fmt.Errorf("list backups: %w", err)
    }
    
    for _, backup := range backups {
        validation := BackupValidation{
            Name:      backup.Key,
            Size:      backup.Size,
            Timestamp: backup.LastModified,
        }
        
        // Download and verify
        data, err := bv.s3Client.Download(ctx, backup.Key)
        if err != nil {
            validation.Status = "FAILED"
            validation.Error = err.Error()
        } else {
            // Verify checksum
            checksum := fmt.Sprintf("%x", md5.Sum(data))
            expectedChecksum := backup.Metadata["checksum"]
            
            if checksum == expectedChecksum {
                validation.Status = "VALID"
                
                // Test restore to temp database
                if err := bv.testRestore(ctx, data); err != nil {
                    validation.Status = "RESTORE_FAILED"
                    validation.Error = err.Error()
                }
            } else {
                validation.Status = "CORRUPT"
                validation.Error = "Checksum mismatch"
            }
        }
        
        report.Backups = append(report.Backups, validation)
        
        // Record metrics
        bv.metrics.RecordBackupValidation(validation)
    }
    
    report.Success = bv.allBackupsValid(report.Backups)
    return report, nil
}

func (bv *BackupValidator) testRestore(ctx context.Context, backupData []byte) error {
    // Create temporary database
    tempDB := fmt.Sprintf("restore_test_%d", time.Now().Unix())
    _, err := bv.db.ExecContext(ctx, fmt.Sprintf("CREATE DATABASE %s", tempDB))
    if err != nil {
        return fmt.Errorf("create temp database: %w", err)
    }
    defer bv.db.ExecContext(ctx, fmt.Sprintf("DROP DATABASE IF EXISTS %s", tempDB))
    
    // Restore backup
    // ... restoration logic ...
    
    // Verify critical tables exist
    tables := []string{"users", "tasks", "events"}
    for _, table := range tables {
        var exists bool
        err := bv.db.QueryRowContext(ctx, `
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = $1
            )
        `, table).Scan(&exists)
        
        if err != nil || !exists {
            return fmt.Errorf("table %s not found after restore", table)
        }
    }
    
    return nil
}
```

## 8. DR Testing Schedule

```yaml
# .github/workflows/dr-test.yml
name: DR Testing
on:
  schedule:
    - cron: '0 2 * * SUN'  # Weekly on Sunday 2am UTC
  workflow_dispatch:       # Manual trigger

jobs:
  dr-drill:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup DR Test Environment
        run: |
          terraform apply -var="environment=dr-test" -auto-approve
          echo "DR_ENV_ID=$GITHUB_RUN_ID" >> $GITHUB_ENV
      
      - name: Deploy Application Stack
        run: |
          helm install mcp-dr ./helm/mcp-ultra-reference \
            --namespace dr-test \
            --values helm/values-dr.yaml \
            --set image.tag=${{ github.sha }}
      
      - name: Run Baseline Workload
        run: |
          k6 run test/load/dr-baseline.js \
            --out influxdb=http://influxdb:8086/k6 \
            --vus 50 --duration 5m
      
      - name: Inject Primary Region Failure
        run: |
          echo "ðŸ”¥ Simulating primary region failure..."
          kubectl delete pods -l region=primary --force --grace-period=0
          kubectl patch deployment primary-app -p '{"spec":{"replicas":0}}'
          echo "FAILURE_TIME=$(date +%s)" >> $GITHUB_ENV
      
      - name: Wait for Automatic Failover
        timeout-minutes: 15
        run: |
          ./scripts/wait-for-failover.sh secondary
      
      - name: Verify Service Recovery
        run: |
          RECOVERY_TIME=$(date +%s)
          RTO=$((RECOVERY_TIME - FAILURE_TIME))
          echo "Recovery completed in ${RTO} seconds"
          
          # Assert RTO compliance
          if [ $RTO -gt 900 ]; then  # 15 minutes
            echo "âŒ RTO exceeded: ${RTO}s > 900s"
            exit 1
          fi
      
      - name: Analyze Data Loss (RPO)
        run: |
          # Get last transaction before failure
          LAST_TX_PRIMARY=$(psql -h $PRIMARY_DB -t -c \
            "SELECT MAX(id) FROM transactions WHERE region='primary'")
          
          # Get first transaction after recovery
          FIRST_TX_SECONDARY=$(psql -h $SECONDARY_DB -t -c \
            "SELECT MIN(id) FROM transactions WHERE region='secondary' 
             AND timestamp > '$FAILURE_TIME'")
          
          LOST_TX=$((FIRST_TX_SECONDARY - LAST_TX_PRIMARY - 1))
          echo "Transactions lost: $LOST_TX"
          
          # Assert RPO compliance (5 min @ 100 tx/sec = 30000 tx max)
          if [ $LOST_TX -gt 30000 ]; then
            echo "âŒ RPO exceeded: ${LOST_TX} transactions lost"
            exit 1
          fi
      
      - name: Test Failback Procedure
        run: |
          echo "Testing failback to primary..."
          ./scripts/dr-failback.sh primary
          
          # Verify primary is accepting traffic again
          curl -f https://primary.example.com/health || exit 1
      
      - name: Generate DR Report
        if: always()
        run: |
          ./scripts/generate-dr-report.sh > dr-report.md
          
          # Upload report
          aws s3 cp dr-report.md \
            s3://dr-reports/$(date +%Y%m%d)-$GITHUB_RUN_ID.md
      
      - name: Cleanup
        if: always()
        run: |
          terraform destroy -var="environment=dr-test" -auto-approve
```

## 9. Recovery Validation Checklist

```go
// internal/dr/validation.go
package dr

type RecoveryValidator struct {
    checks []ValidationCheck
}

func (rv *RecoveryValidator) ValidateRecovery(ctx context.Context) (*ValidationReport, error) {
    report := &ValidationReport{
        Timestamp: time.Now(),
        Checks:    make([]CheckResult, 0),
    }
    
    for _, check := range rv.checks {
        result := CheckResult{
            Name:      check.Name,
            StartTime: time.Now(),
        }
        
        err := check.Execute(ctx)
        result.EndTime = time.Now()
        result.Duration = result.EndTime.Sub(result.StartTime)
        
        if err != nil {
            result.Status = "FAILED"
            result.Error = err.Error()
        } else {
            result.Status = "PASSED"
        }
        
        report.Checks = append(report.Checks, result)
    }
    
    report.Success = rv.allChecksPassed(report.Checks)
    return report, nil
}

// Default validation checks
var defaultChecks = []ValidationCheck{
    {
        Name: "Database Connectivity",
        Execute: func(ctx context.Context) error {
            db, err := sql.Open("postgres", os.Getenv("DATABASE_URL"))
            if err != nil {
                return err
            }
            defer db.Close()
            return db.PingContext(ctx)
        },
    },
    {
        Name: "Data Integrity",
        Execute: func(ctx context.Context) error {
            // Verify critical data checksums
            expected := "a3f2b1c4d5e6f7g8"
            actual := calculateDataChecksum(ctx)
            if actual != expected {
                return fmt.Errorf("checksum mismatch: expected %s, got %s", expected, actual)
            }
            return nil
        },
    },
    {
        Name: "NATS Streaming",
        Execute: func(ctx context.Context) error {
            nc, err := nats.Connect(os.Getenv("NATS_URL"))
            if err != nil {
                return err
            }
            defer nc.Close()
            
            js, err := nc.JetStream()
            if err != nil {
                return err
            }
            
            // Verify streams are accessible
            streams := []string{"events", "commands", "audit"}
            for _, stream := range streams {
                if _, err := js.StreamInfo(stream); err != nil {
                    return fmt.Errorf("stream %s not accessible: %w", stream, err)
                }
            }
            return nil
        },
    },
    {
        Name: "API Health",
        Execute: func(ctx context.Context) error {
            endpoints := []string{
                "/healthz",
                "/readyz",
                "/api/v1/status",
            }
            
            client := &http.Client{Timeout: 5 * time.Second}
            for _, endpoint := range endpoints {
                resp, err := client.Get("http://localhost:9655" + endpoint)
                if err != nil {
                    return fmt.Errorf("endpoint %s failed: %w", endpoint, err)
                }
                if resp.StatusCode != http.StatusOK {
                    return fmt.Errorf("endpoint %s returned %d", endpoint, resp.StatusCode)
                }
            }
            return nil
        },
    },
    {
        Name: "Critical Transactions",
        Execute: func(ctx context.Context) error {
            // Test critical business transactions
            tests := []func() error{
                testUserAuthentication,
                testPaymentProcessing,
                testDataCreation,
            }
            
            for _, test := range tests {
                if err := test(); err != nil {
                    return err
                }
            }
            return nil
        },
    },
    {
        Name: "Monitoring Pipeline",
        Execute: func(ctx context.Context) error {
            // Verify metrics are being collected
            resp, err := http.Get("http://prometheus:9090/api/v1/query?query=up")
            if err != nil {
                return fmt.Errorf("prometheus not accessible: %w", err)
            }
            defer resp.Body.Close()
            
            var result map[string]interface{}
            if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
                return fmt.Errorf("invalid prometheus response: %w", err)
            }
            
            if result["status"] != "success" {
                return fmt.Errorf("prometheus query failed")
            }
            return nil
        },
    },
}
```

## 10. DR Metrics & Monitoring

```go
// internal/dr/metrics.go
package dr

import (
    "github.com/prometheus/client_golang/prometheus"
)

type DRMetrics struct {
    backupDuration      *prometheus.HistogramVec
    backupSize         *prometheus.GaugeVec
    replicationLag     *prometheus.GaugeVec
    failoverDuration   prometheus.Histogram
    failoverCount      *prometheus.CounterVec
    recoveryValidation *prometheus.GaugeVec
    rtoCompliance      prometheus.Gauge
    rpoCompliance      prometheus.Gauge
}

func NewDRMetrics() *DRMetrics {
    return &DRMetrics{
        backupDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name: "dr_backup_duration_seconds",
                Help: "Duration of backup operations",
                Buckets: prometheus.ExponentialBuckets(10, 2, 10),
            },
            []string{"type", "status"},
        ),
        
        backupSize: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "dr_backup_size_bytes",
                Help: "Size of backups in bytes",
            },
            []string{"type", "location"},
        ),
        
        replicationLag: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "dr_replication_lag_seconds",
                Help: "Replication lag in seconds",
            },
            []string{"source", "target"},
        ),
        
        failoverDuration: prometheus.NewHistogram(
            prometheus.HistogramOpts{
                Name: "dr_failover_duration_seconds",
                Help: "Duration of failover operations",
                Buckets: []float64{30, 60, 120, 300, 600, 900, 1800},
            },
        ),
        
        failoverCount: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "dr_failover_total",
                Help: "Total number of failovers",
            },
            []string{"from_region", "to_region", "reason"},
        ),
        
        recoveryValidation: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "dr_recovery_validation_status",
                Help: "Status of recovery validation checks (1=pass, 0=fail)",
            },
            []string{"check"},
        ),
        
        rtoCompliance: prometheus.NewGauge(
            prometheus.GaugeOpts{
                Name: "dr_rto_compliance_percent",
                Help: "Percentage of failovers meeting RTO",
            },
        ),
        
        rpoCompliance: prometheus.NewGauge(
            prometheus.GaugeOpts{
                Name: "dr_rpo_compliance_percent",
                Help: "Percentage of recoveries meeting RPO",
            },
        ),
    }
}
```

## 11. Configuration

```yaml
# config/dr.yaml
disaster_recovery:
  backup:
    schedule: "0 */6 * * *"
    retention_days: 30
    locations:
      - s3://backup-primary/
      - s3://backup-secondary/
    validation:
      enabled: true
      frequency: daily
  
  replication:
    mode: streaming
    max_lag_seconds: 30
    replicas:
      - region: us-east-1
        priority: 100
      - region: eu-west-1
        priority: 90
      - region: us-west-2
        priority: 50
  
  failover:
    automatic: true
    health_check_interval: 30s
    failure_threshold: 3
    dns_ttl: 60
    connection_drain_timeout: 30s
  
  testing:
    enabled: true
    schedule: weekly
    scenarios:
      - region_failure
      - database_corruption
      - network_partition
      - backup_restore
  
  monitoring:
    alerts:
      - name: high_replication_lag
        threshold: 60s
        severity: warning
      - name: backup_failure
        threshold: 2
        severity: critical
      - name: rto_violation
        threshold: 900s
        severity: critical
```

## 12. Summary

Este plano de DR garante:

âœ… **RTO < 15 minutos** para serviÃ§os crÃ­ticos
âœ… **RPO < 5 minutos** com replicaÃ§Ã£o streaming
âœ… **Backups automatizados** com validaÃ§Ã£o
âœ… **Failover multi-regiÃ£o** automÃ¡tico
âœ… **Testes semanais** de DR
âœ… **Runbooks automatizados** para resposta rÃ¡pida
âœ… **Monitoramento contÃ­nuo** de saÃºde e compliance

O sistema estÃ¡ preparado para:
- Falhas de regiÃ£o completa
- CorrupÃ§Ã£o de dados
- Particionamento de rede
- Perda de datacenter

Com validaÃ§Ã£o contÃ­nua e testes automatizados, garantimos que o DR funcionarÃ¡ quando necessÃ¡rio.